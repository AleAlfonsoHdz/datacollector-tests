# Copyright 2021 StreamSets Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import json
import logging
import string

import pytest
from streamsets.testframework.markers import gcp, sdc_min_version
from streamsets.testframework.utils import get_random_string


logger = logging.getLogger(__name__)

TMPOUT = '/tmp/out/'
PROTOBUF_FILE_PATH = 'resources/protobuf/addressbook.desc'


@gcp
def test_data_types(sdc_builder, sdc_executor, gcp):
    pytest.skip("GCS stores objects without doing data type distinctions.")


#Reference: https://cloud.google.com/storage/docs/naming-buckets
GCS_BUCKET_NAMES = [
    ('max_size', "stf_std_" + get_random_string(string.ascii_lowercase, 55)),
        # Max length is 63, minus the 8 characters from the prefix
    ('lowercase', "stf_std_" + get_random_string(string.ascii_lowercase, 20)),
    ('numbers', "stf_std_" + get_random_string(string.ascii_lowercase, 5) + "1234567890" + get_random_string(string.ascii_lowercase, 5)),
    ('special', "stf_std_" + get_random_string(string.ascii_lowercase, 5) + "-_" + get_random_string(string.ascii_lowercase, 5))
]

@gcp
@pytest.mark.parametrize('test_name,bucket_name', GCS_BUCKET_NAMES, ids=[i[0] for i in GCS_BUCKET_NAMES])
def test_bucket_names(sdc_builder, sdc_executor, gcp, test_name, bucket_name):
    """
    Write data to Google cloud storage with different valid bucket names.

    The pipeline looks like:
        dev_raw_data_source >> google_cloud_storage_destination
    """
    pipeline_builder = sdc_builder.get_pipeline_builder()

    storage_client = gcp.storage_client

    data = [get_random_string(string.ascii_letters, length=100) for _ in range(10)]

    dev_raw_data_source = pipeline_builder.add_stage('Dev Raw Data Source')

    dev_raw_data_source.set_attributes(data_format='TEXT',
                                       stop_after_first_batch=True,
                                       raw_data='\n'.join(data))

    google_cloud_storage = pipeline_builder.add_stage('Google Cloud Storage', type='destination')

    google_cloud_storage.set_attributes(bucket=bucket_name,
                                        common_prefix='gcs-test',
                                        partition_prefix='test',
                                        data_format='TEXT')

    dev_raw_data_source >> google_cloud_storage

    pipeline = pipeline_builder.build(title=f'Google Cloud Storage Destination Bucket Names {test_name}').configure_for_environment(gcp)
    sdc_executor.add_pipeline(pipeline)

    try:
        created_bucket = gcp.retry_429(storage_client.create_bucket)(bucket_name)

        logger.info('Starting GCS Destination pipeline and waiting for it to produce records'
                    ' and transition to finished...')
        sdc_executor.start_pipeline(pipeline).wait_for_finished()

        blob_iter = created_bucket.list_blobs(max_results=1, prefix='gcs-test')
        blobs = [blob for blob in blob_iter]
        assert len(blobs) == 1
        blob = blobs[0]
        # Decode the byte array returned by storage client
        contents = blob.download_as_string().decode('ascii')
        # Strip out the lines which are empty (essentially the last line)
        lines = [line for line in contents.split('\n') if len(line) > 0]
        assert lines == data

    finally:
        logger.info('Deleting bucket %s ...', created_bucket.name)
        gcp.retry_429(created_bucket.delete)(force=True)


#Reference: https://cloud.google.com/storage/docs/naming-objects
GCS_OBJECT_NAMES = [
    ('max_size', get_random_string(string.ascii_lowercase, 974)),
        # Max length for GCS object is 1024 chars, minus the prefix gcs-test/ and the length of the file generated by SDC
    ('min_size', get_random_string(string.ascii_lowercase, 1)),
    ('lowercase', get_random_string(string.ascii_lowercase, 20)),
    ('uppercase', get_random_string(string.ascii_uppercase, 20)),
    ('mixedcase', get_random_string(string.ascii_letters, 20) + get_random_string(string.ascii_letters, 20)),
    ('numbers', get_random_string(string.ascii_letters, 5) + "1234567890" + get_random_string(string.ascii_letters, 5)),
    ('special', get_random_string(string.ascii_letters, 5) + "!|@·#$%&/()='?¡¿[]`^+*{}´¨,.;:-_" + get_random_string(string.ascii_letters, 5))
]

@gcp
@pytest.mark.parametrize('test_name,object_name', GCS_OBJECT_NAMES, ids=[i[0] for i in GCS_OBJECT_NAMES])
def test_object_names(sdc_builder, sdc_executor, gcp, test_name, object_name):
    """
    Write data to Google cloud storage with different valid object (blob) names.

    The pipeline looks like:
        dev_raw_data_source >> google_cloud_storage_destination
    """
    pipeline_builder = sdc_builder.get_pipeline_builder()

    storage_client = gcp.storage_client

    bucket_name = "stf_std_" + get_random_string(string.ascii_lowercase, 20)

    data = [get_random_string(string.ascii_letters, length=100) for _ in range(10)]

    dev_raw_data_source = pipeline_builder.add_stage('Dev Raw Data Source')

    dev_raw_data_source.set_attributes(data_format='TEXT',
                                       stop_after_first_batch=True,
                                       raw_data='\n'.join(data))

    google_cloud_storage = pipeline_builder.add_stage('Google Cloud Storage', type='destination')

    google_cloud_storage.set_attributes(bucket=bucket_name,
                                        common_prefix='gcs-test',
                                        partition_prefix=object_name,
                                        data_format='TEXT')

    dev_raw_data_source >> google_cloud_storage

    pipeline = pipeline_builder.build(
        title=f'Google Cloud Storage Destination Object Names {test_name}').configure_for_environment(gcp)
    sdc_executor.add_pipeline(pipeline)

    try:
        created_bucket = gcp.retry_429(storage_client.create_bucket)(bucket_name)
        sdc_executor.start_pipeline(pipeline).wait_for_finished()

        blob_iter = created_bucket.list_blobs(max_results=1, prefix='gcs-test')
        blobs = [blob for blob in blob_iter]
        assert len(blobs) == 1
        blob = blobs[0]
        # Decode the byte array returned by storage client
        contents = blob.download_as_string().decode('ascii')
        # Strip out the lines which are empty (essentially the last line)
        lines = [line for line in contents.split('\n') if len(line) > 0]
        assert lines == data
    finally:
        logger.info('Deleting bucket %s ...', created_bucket.name)
        gcp.retry_429(created_bucket.delete)(force=True)


@gcp
def test_dataflow_events(sdc_builder, sdc_executor, gcp):
    """
    Write data to Google cloud storage, capture events on wiretap and verify their content

    The pipeline looks like:
        dev_raw_data_source >> google_cloud_storage_origin >= wiretap
    """
    pipeline_builder = sdc_builder.get_pipeline_builder()

    storage_client = gcp.storage_client

    bucket_name = "stf_std_" + get_random_string(string.ascii_lowercase, 20)

    data = [get_random_string(string.ascii_letters, length=100) for _ in range(10)]

    dev_raw_data_source = pipeline_builder.add_stage('Dev Raw Data Source')

    dev_raw_data_source.set_attributes(data_format='TEXT',
                                       stop_after_first_batch=True,
                                       raw_data='\n'.join(data))

    google_cloud_storage = pipeline_builder.add_stage('Google Cloud Storage', type='destination')

    google_cloud_storage.set_attributes(bucket=bucket_name,
                                        common_prefix='gcs-test',
                                        partition_prefix='${YYYY()}/${MM()}/${DD()}/${hh()}/${mm()}',
                                        data_format='TEXT')

    wiretap = pipeline_builder.add_wiretap()

    dev_raw_data_source >> google_cloud_storage >= wiretap.destination

    pipeline = pipeline_builder.build(
        title=f'Google Cloud Storage Destination Dataflow Events').configure_for_environment(gcp)
    sdc_executor.add_pipeline(pipeline)

    try:
        created_bucket = gcp.retry_429(storage_client.create_bucket)(bucket_name)
        sdc_executor.start_pipeline(pipeline).wait_for_finished()

        blob_iter = created_bucket.list_blobs(max_results=1, prefix='gcs-test')
        blobs = [blob for blob in blob_iter]
        assert len(blobs) == 1
        blob = blobs[0]
        # Decode the byte array returned by storage client
        contents = blob.download_as_string().decode('ascii')
        # Strip out the lines which are empty (essentially the last line)
        lines = [line for line in contents.split('\n') if len(line) > 0]
        assert lines == data

        # One event should be created
        records = wiretap.output_records
        assert len(records) == 1
        assert records[0].get_field_data('/bucket') == bucket_name
        assert records[0].get_field_data('/recordCount') == 10

    finally:
        logger.info('Deleting bucket %s ...', created_bucket.name)
        gcp.retry_429(created_bucket.delete)(force=True)


@gcp
def test_multiple_batches(sdc_builder, sdc_executor, gcp):
    """
    Test that we can write multiple batches and the pipeline writes all the data exactly once.

    The pipeline looks like:
        dev_raw_data_source >> google_cloud_storage_destination
    """
    pipeline_builder = sdc_builder.get_pipeline_builder()

    storage_client = gcp.storage_client

    bucket_name = "stf_std_" + get_random_string(string.ascii_lowercase, 20)

    batch_size = 100
    batches = 10

    dev_data_generator = pipeline_builder.add_stage('Dev Data Generator')

    dev_data_generator.set_attributes(batch_size=batch_size,
                                      records_to_be_generated=batch_size * batches,
                                      fields_to_generate=[
                                          {'field': 'text', 'type': 'LONG_SEQUENCE'}])

    google_cloud_storage = pipeline_builder.add_stage('Google Cloud Storage', type='destination')

    google_cloud_storage.set_attributes(bucket=bucket_name,
                                        common_prefix='gcs-test',
                                        partition_prefix='test',
                                        data_format='TEXT')

    dev_data_generator >> google_cloud_storage

    pipeline = pipeline_builder.build(
        title=f'Google Cloud Storage Destination Test Multiple Batches').configure_for_environment(gcp)
    sdc_executor.add_pipeline(pipeline)


    try:
        created_bucket = gcp.retry_429(storage_client.create_bucket)(bucket_name)

        sdc_executor.start_pipeline(pipeline).wait_for_pipeline_output_records_count(batch_size * batches)
        sdc_executor.stop_pipeline(pipeline)

        # Now the pipeline will write some amount of records that will be larger, so we get precise count from metrics
        history = sdc_executor.get_pipeline_history(pipeline)
        records = history.latest.metrics.counter('pipeline.batchInputRecords.counter').count
        logger.info(f"Detected {records} output records")
        # Sanity check
        assert records == batch_size * batches, "Check that no records have been lost/duplicated"

        # GCS creates writes each batch in a different blob. Therefore, there should be 10 blobs, with 100 records each
        blob_iter = created_bucket.list_blobs(prefix='gcs-test')
        blobs = [blob for blob in blob_iter]
        assert len(blobs) >= batches, "GCS puts each batch in a different blob, so there are at least 10 blobs but" \
                                      "potentially more if incomplete batches were generated"
        result = []
        for blob in blobs:
            # Decode the byte array returned by storage client
            contents = blob.download_as_string().decode('ascii')
            # Strip out the lines which are empty (essentially the last line)
            lines = [line for line in contents.split('\n') if len(line) > 0]
            # Each blob should have one batch of elements
            assert len(lines) == batch_size, "Every blob must have a full batch"
            result.extend(lines)

        result.sort(key=float)
        assert result == [f'{i}' for i in range(0, records)], "Verify that every record has been written exactly once " \
                                                              "by checking the sequence"

    finally:
        logger.info('Deleting bucket %s ...', created_bucket.name)
        gcp.retry_429(created_bucket.delete)(force=True)


@gcp
def test_data_format_avro(sdc_builder, sdc_executor, gcp):
    """
    Write data to Google cloud storage using Avro format.

    The pipeline looks like:
        google_cloud_storage_origin >> wiretap
    """
    DATA = {'name': 'boss', 'age': 60, 'emails': ['boss@company.com', 'boss2@company.com'], 'boss': None}
    SCHEMA = {'namespace': 'example.avro',
              'type': 'record',
              'name': 'Employee',
              'fields': [{'name': 'name', 'type': 'string'},
                         {'name': 'age', 'type': 'int'},
                         {'name': 'emails', 'type': {'type': 'array', 'items': 'string'}},
                         {'name': 'boss', 'type': ['Employee', 'null']}]}

    pipeline_builder = sdc_builder.get_pipeline_builder()

    storage_client = gcp.storage_client

    bucket_name = "stf_std_" + get_random_string(string.ascii_lowercase, 20)


    dev_raw_data_source = pipeline_builder.add_stage('Dev Raw Data Source')

    dev_raw_data_source.set_attributes(data_format='JSON',
                                       stop_after_first_batch=True,
                                       raw_data=json.dumps(DATA))

    google_cloud_storage = pipeline_builder.add_stage('Google Cloud Storage', type='destination')

    google_cloud_storage.set_attributes(bucket=bucket_name,
                                        common_prefix='gcs-test',
                                        partition_prefix='test',
                                        data_format='AVRO',
                                        avro_schema=json.dumps(SCHEMA),
                                        avro_schema_location='INLINE')

    dev_raw_data_source >> google_cloud_storage

    pipeline = pipeline_builder.build(
        title=f'Google Cloud Storage Destination Data Format Avro').configure_for_environment(gcp)
    sdc_executor.add_pipeline(pipeline)

    try:
        created_bucket = gcp.retry_429(storage_client.create_bucket)(bucket_name)

        logger.info('Starting GCS Origin pipeline and wait until the information is read ...')
        sdc_executor.start_pipeline(pipeline).wait_for_finished()

        # To verify that the Avro format has been successfully stored, we read the data from GCS using an auxiliary
        # pipeline. This pipeline is set to read data in Avro format.
        result = read_avro_data(bucket_name, sdc_builder, sdc_executor, gcp)

        # We compare the results read by the GCS Origin pipeline and check that the data is equal to the original data stored
        assert [record.field for record in result] == [DATA]
    finally:
        logger.info('Deleting bucket %s ...', created_bucket.name)
        gcp.retry_429(created_bucket.delete)(force=True)


@gcp
@sdc_min_version('3.22.0')
def test_data_format_delimited(sdc_builder, sdc_executor, gcp):
    """
    Write data to Google cloud storage using Delimited format.

    The pipeline looks like:
        dev_raw_data_source >> google_cloud_storage_destination
    """
    pipeline_builder = sdc_builder.get_pipeline_builder()

    storage_client = gcp.storage_client

    bucket_name = "stf_std_" + get_random_string(string.ascii_lowercase, 20)

    raw_data = 'Alex,Xavi,Tucu,Martin'
    EXPECTED_OUTPUT = 'Alex,Xavi,Tucu,Martin\r'

    dev_raw_data_source = pipeline_builder.add_stage('Dev Raw Data Source').set_attributes(data_format='DELIMITED',
                                                                                  raw_data=raw_data,
                                                                                  stop_after_first_batch=True
                                                                                  )

    google_cloud_storage = pipeline_builder.add_stage('Google Cloud Storage', type='destination')

    google_cloud_storage.set_attributes(bucket=bucket_name,
                                        common_prefix='gcs-test',
                                        partition_prefix='test',
                                        data_format='DELIMITED')

    dev_raw_data_source >> google_cloud_storage

    pipeline = pipeline_builder.build(
        title=f'Google Cloud Storage Destination Test Data Format Delimited').configure_for_environment(gcp)
    sdc_executor.add_pipeline(pipeline)


    try:
        created_bucket = gcp.retry_429(storage_client.create_bucket)(bucket_name)

        sdc_executor.start_pipeline(pipeline).wait_for_finished()

        blob_iter = created_bucket.list_blobs(max_results=1, prefix='gcs-test')
        blobs = [blob for blob in blob_iter]
        assert len(blobs) == 1
        blob = blobs[0]
        # Decode the byte array returned by storage client
        contents = blob.download_as_string().decode('ascii')
        # Strip out the lines which are empty (essentially the last line)
        lines = [line for line in contents.split('\n') if len(line) > 0]
        assert len(lines) == 1
        assert lines[0] == EXPECTED_OUTPUT

    finally:
        logger.info('Deleting bucket %s ...', created_bucket.name)
        gcp.retry_429(created_bucket.delete)(force=True)


@gcp
@pytest.mark.parametrize('data_type', ['ARRAY', 'ARRAY_OF_OBJECTS', 'OBJECT'])
def test_data_format_json(sdc_builder, sdc_executor, gcp, data_type):
    """
    Write data to Google cloud storage using JSON format.

    The pipeline looks like:
        dev_raw_data_source >> google_cloud_storage_destination
    """
    pipeline_builder = sdc_builder.get_pipeline_builder()

    storage_client = gcp.storage_client

    bucket_name = "stf_std_" + get_random_string(string.ascii_lowercase, 20)

    raw_data = f'{{ "f1" : "abc", "f2" : "xyz"}}'

    dev_raw_data_source = pipeline_builder.add_stage('Dev Raw Data Source').set_attributes(data_format='JSON',
                                                                                  raw_data=raw_data,
                                                                                  stop_after_first_batch=True)

    google_cloud_storage = pipeline_builder.add_stage('Google Cloud Storage', type='destination')

    google_cloud_storage.set_attributes(bucket=bucket_name,
                                        common_prefix='gcs-test',
                                        partition_prefix='test',
                                        data_format='JSON')

    dev_raw_data_source >> google_cloud_storage

    pipeline = pipeline_builder.build(
        title=f'Google Cloud Storage Destination Test Data Format JSON').configure_for_environment(gcp)
    sdc_executor.add_pipeline(pipeline)


    try:
        created_bucket = gcp.retry_429(storage_client.create_bucket)(bucket_name)

        sdc_executor.start_pipeline(pipeline).wait_for_finished()

        blob_iter = created_bucket.list_blobs(max_results=1, prefix='gcs-test')
        blobs = [blob for blob in blob_iter]
        assert len(blobs) == 1
        blob = blobs[0]
        # Decode the byte array returned by storage client
        contents = blob.download_as_string().decode('ascii')
        # Strip out the lines which are empty (essentially the last line)
        lines = [line for line in contents.split('\n') if len(line) > 0]
        assert len(lines) == 1
        assert json.loads(lines[0]) == json.loads(raw_data)

    finally:
        logger.info('Deleting bucket %s ...', created_bucket.name)
        gcp.retry_429(created_bucket.delete)(force=True)


@gcp
def test_data_format_protobuf(sdc_builder, sdc_executor, gcp):
    """
    Write data to Google cloud storage using Protobuf format.

    The pipeline looks like:
        dev_raw_data_source >> google_cloud_storage_destination
    """
    data = '{"first_name": "Martin","last_name": "Balzamo"}'
    expected = '\x11\x06Martin\x12\x07Balzamo'

    pipeline_builder = sdc_builder.get_pipeline_builder()

    storage_client = gcp.storage_client
    bucket_name = "stf_std_" + get_random_string(string.ascii_lowercase, 20)

    dev_raw_data_source = pipeline_builder.add_stage('Dev Raw Data Source')

    dev_raw_data_source.set_attributes(data_format='JSON',
                                       stop_after_first_batch=True,
                                       raw_data=data)

    google_cloud_storage = pipeline_builder.add_stage('Google Cloud Storage', type='destination')
    google_cloud_storage.set_attributes(bucket=bucket_name,
                                        common_prefix='gcs-test',
                                        data_format='PROTOBUF',
                                        message_type='Contact',
                                        protobuf_descriptor_file=PROTOBUF_FILE_PATH)

    dev_raw_data_source >> google_cloud_storage

    pipeline = pipeline_builder.build(
        title=f'Google Cloud Storage Destination Data Format Protobuf').configure_for_environment(gcp)
    sdc_executor.add_pipeline(pipeline)

    try:
        created_bucket = gcp.retry_429(storage_client.create_bucket)(bucket_name)

        sdc_executor.start_pipeline(pipeline).wait_for_finished()

        blob_iter = created_bucket.list_blobs(max_results=1, prefix='gcs-test')
        blobs = [blob for blob in blob_iter]
        assert len(blobs) == 1
        blob = blobs[0]
        # Decode the byte array returned by storage client
        contents = blob.download_as_string().decode('ascii')
        # Strip out the lines which are empty (essentially the last line)
        lines = [line for line in contents.split('\n') if len(line) > 0]
        result = ''.join(lines)
        assert result == expected
    finally:
        logger.info('Deleting bucket %s ...', created_bucket.name)
        gcp.retry_429(created_bucket.delete)(force=True)


@gcp
def test_data_format_sdc_record(sdc_builder, sdc_executor, gcp):
    """
    Write data to Google cloud storage using SDC Record format.

    The pipeline looks like:
        dev_raw_data_source >> google_cloud_storage_destination
    """
    json_data = [{"field1": "abc", "field2": "def", "field3": "ghi"},
                 {"field1": "jkl", "field2": "mno", "field3": "pqr"}]
    raw_data = ''.join(json.dumps(record) for record in json_data)

    pipeline_builder = sdc_builder.get_pipeline_builder()

    storage_client = gcp.storage_client

    bucket_name = "stf_std_" + get_random_string(string.ascii_lowercase, 20)

    dev_raw_data_source = pipeline_builder.add_stage('Dev Raw Data Source').set_attributes(data_format='JSON',
                                                                                  raw_data=raw_data,
                                                                                  stop_after_first_batch=True)

    google_cloud_storage = pipeline_builder.add_stage('Google Cloud Storage', type='destination')

    google_cloud_storage.set_attributes(bucket=bucket_name,
                                        common_prefix='gcs-test',
                                        partition_prefix='test',
                                        data_format='SDC_JSON')

    dev_raw_data_source >> google_cloud_storage

    pipeline = pipeline_builder.build(
        title=f'Google Cloud Storage Destination Test Data Format SDC Record').configure_for_environment(gcp)
    sdc_executor.add_pipeline(pipeline)

    try:
        created_bucket = gcp.retry_429(storage_client.create_bucket)(bucket_name)

        sdc_executor.start_pipeline(pipeline).wait_for_finished()

        # To verify that the SDC Record format has been successfully stored, we read the data from GCS using an auxiliary
        # pipeline. This pipeline is set to read data in SDC Record format.
        wiretap_records = read_messages_SDC_record_gcp(bucket_name, sdc_builder, sdc_executor, gcp)

        # We compare the results read by the GCS Origin pipeline and check that the data is equal to the original data stored
        assert len(wiretap_records) == len(json_data)
        assert wiretap_records[0].field == json_data[0]
        assert wiretap_records[1].field == json_data[1]

    finally:
        logger.info('Deleting bucket %s ...', created_bucket.name)
        gcp.retry_429(created_bucket.delete)(force=True)


@gcp
def test_data_format_text(sdc_builder, sdc_executor, gcp):
    """
    Write data to Google cloud storage using text format.

    The pipeline looks like:
        dev_raw_data_source >> google_cloud_storage_destination
    """
    pipeline_builder = sdc_builder.get_pipeline_builder()

    storage_client = gcp.storage_client

    bucket_name = "stf_std_" + get_random_string(string.ascii_lowercase, 20)

    data = [get_random_string(string.ascii_letters, length=100) for _ in range(10)]

    dev_raw_data_source = pipeline_builder.add_stage('Dev Raw Data Source')

    dev_raw_data_source.set_attributes(data_format='TEXT',
                                       stop_after_first_batch=True,
                                       raw_data='\n'.join(data))

    google_cloud_storage = pipeline_builder.add_stage('Google Cloud Storage', type='destination')

    google_cloud_storage.set_attributes(bucket=bucket_name,
                                        common_prefix='gcs-test',
                                        partition_prefix='test',
                                        data_format='TEXT')

    dev_raw_data_source >> google_cloud_storage

    pipeline = pipeline_builder.build(
        title=f'Google Cloud Storage Destination Test Data Format Text').configure_for_environment(gcp)
    sdc_executor.add_pipeline(pipeline)

    try:
        created_bucket = gcp.retry_429(storage_client.create_bucket)(bucket_name)
        sdc_executor.start_pipeline(pipeline).wait_for_finished()

        blob_iter = created_bucket.list_blobs(max_results=1, prefix='gcs-test')
        blobs = [blob for blob in blob_iter]
        assert len(blobs) == 1
        blob = blobs[0]
        # Decode the byte array returned by storage client
        contents = blob.download_as_string().decode('ascii')
        # Strip out the lines which are empty (essentially the last line)
        lines = [line for line in contents.split('\n') if len(line) > 0]
        assert lines == data
    finally:
        logger.info('Deleting bucket %s ...', created_bucket.name)
        gcp.retry_429(created_bucket.delete)(force=True)


@gcp
def test_data_format_whole_file(sdc_builder, sdc_executor, gcp):
    """
    Write file to LocalFS.
    Then, read the data and write to GCS using whole file format.

    The pipeline looks like:
        localFS >> google_cloud_storage_destination
    """
    pipeline_builder = sdc_builder.get_pipeline_builder()

    storage_client = gcp.storage_client

    data = [get_random_string(string.ascii_letters, length=100) for _ in range(10)]
    bucket_name = "stf_std_" + get_random_string(string.ascii_lowercase, 20)
    gcp_file = get_random_string(string.ascii_lowercase, 20)


    directory = pipeline_builder.add_stage('Directory', type='origin')
    directory.set_attributes(data_format='WHOLE_FILE', file_name_pattern=f'{gcp_file}*', file_name_pattern_mode='GLOB',
                             file_post_processing='DELETE', files_directory=TMPOUT,
                             process_subdirectories=False, read_order='TIMESTAMP')

    google_cloud_storage = pipeline_builder.add_stage('Google Cloud Storage', type='destination')
    google_cloud_storage.set_attributes(bucket=bucket_name,
                                        common_prefix='gcs-test',
                                        data_format='WHOLE_FILE',
                                        file_name_expression=gcp_file)

    pipeline_finished_executor = pipeline_builder.add_stage('Pipeline Finisher Executor')
    pipeline_finished_executor.set_attributes(
        stage_record_preconditions=["${record:eventType() == 'no-more-data'}"])

    directory >> google_cloud_storage
    directory >= pipeline_finished_executor

    pipeline = pipeline_builder.build(
        title=f'Google Cloud Storage Destination Test Data Format Whole File').configure_for_environment(gcp)

    try:
        created_bucket = gcp.retry_429(storage_client.create_bucket)(bucket_name)

        write_whole_file_to_LocalFS(data, gcp_file, sdc_builder, sdc_executor, gcp)

        sdc_executor.add_pipeline(pipeline)
        sdc_executor.start_pipeline(pipeline).wait_for_finished()

        blob_iter = created_bucket.list_blobs(max_results=1, prefix='gcs-test')
        blobs = [blob for blob in blob_iter]
        assert len(blobs) == 1
        blob = blobs[0]
        # Decode the byte array returned by storage client
        contents = blob.download_as_string().decode('ascii')
        # Strip out the lines which are empty (essentially the last line)
        lines = [line for line in contents.split('\n') if len(line) > 0]
        assert lines == data
    finally:
        logger.info('Deleting bucket %s ...', created_bucket.name)
        gcp.retry_429(created_bucket.delete)(force=True)


@gcp
def test_multithreading(sdc_builder, sdc_executor, gcp):
    pytest.skip("Multithreading not supported for GCS Origin")


@gcp
def test_push_pull(sdc_builder, sdc_executor, gcp):
    """
    We plan to verify that the connector works fine with Dev Raw Data Source and Dev Data Generator, an example of pull
    and push strategies, so as we already verified Dev Raw Data Source, we will use Dev Data Generator here to complete
    the coverage.

    The pipeline looks like:
        dev_raw_data_source >> google_cloud_storage_destination
    """

    pipeline_builder = sdc_builder.get_pipeline_builder()

    storage_client = gcp.storage_client

    bucket_name = "stf_std_" + get_random_string(string.ascii_lowercase, 20)

    dev_data_generator = pipeline_builder.add_stage('Dev Data Generator')

    dev_data_generator.set_attributes(batch_size=1,
                                      fields_to_generate=[
                                          {'field': 'stringField', 'type': 'STRING', 'precision': 10, 'scale': 2}])

    google_cloud_storage = pipeline_builder.add_stage('Google Cloud Storage', type='destination')

    google_cloud_storage.set_attributes(bucket=bucket_name,
                                        common_prefix='gcs-test',
                                        partition_prefix='test',
                                        data_format='JSON')

    dev_data_generator >> google_cloud_storage

    pipeline = pipeline_builder.build(
        title=f'Google Cloud Storage Destination Test Push Pull').configure_for_environment(gcp)
    sdc_executor.add_pipeline(pipeline)

    try:
        created_bucket = gcp.retry_429(storage_client.create_bucket)(bucket_name)

        sdc_executor.start_pipeline(pipeline).wait_for_pipeline_output_records_count(20)
        sdc_executor.stop_pipeline(pipeline)

        history = sdc_executor.get_pipeline_history(pipeline)
        history_records = history.latest.metrics.counter('stage.GoogleCloudStorage_01.outputRecords.counter').count

        blob_iter = created_bucket.list_blobs(prefix='gcs-test')
        blobs = [blob for blob in blob_iter]
        assert len(blobs) == history_records

    finally:
        logger.info('Deleting bucket %s ...', created_bucket.name)
        gcp.retry_429(created_bucket.delete)(force=True)


def read_messages_SDC_record_gcp(bucket_name, sdc_builder, sdc_executor, gcp):
    # Read SDC Records from GCS Origin. This is used to verify that the SDC Record format has been succesfully stored
    # in test_data_format_sdc_record.

    pipeline_builder = sdc_builder.get_pipeline_builder()

    google_cloud_storage = pipeline_builder.add_stage('Google Cloud Storage', type='origin')

    google_cloud_storage.set_attributes(bucket=bucket_name,
                                        common_prefix='gcs-test',
                                        prefix_pattern='**/*',
                                        data_format='SDC_JSON')
    wiretap = pipeline_builder.add_wiretap()

    google_cloud_storage >> wiretap.destination

    pipeline = pipeline_builder.build(itle=f'GCS Origin Reader').configure_for_environment(gcp)
    sdc_executor.add_pipeline(pipeline)

    sdc_executor.start_pipeline(pipeline).wait_for_pipeline_output_records_count(2)
    sdc_executor.stop_pipeline(pipeline)

    return wiretap.output_records


def write_whole_file_to_LocalFS(data, gcp_file, sdc_builder, sdc_executor, gcp):
    # Generate a file in LocalFS. This file will be used by test_format_whole_file to verify that an entire file
    # can be moved to GCS.

    pipeline_builder = sdc_builder.get_pipeline_builder()
    pipeline_builder.add_error_stage('Discard')

    dev_raw_data_source = pipeline_builder.add_stage('Dev Raw Data Source')

    dev_raw_data_source.set_attributes(data_format='TEXT',
                                       stop_after_first_batch=True,
                                       raw_data='\n'.join(data))

    local_fs = pipeline_builder.add_stage('Local FS', type='destination')

    local_fs.set_attributes(directory_template=TMPOUT, data_format='TEXT', files_prefix=gcp_file)

    dev_raw_data_source >> local_fs

    pipeline = pipeline_builder.build(
        title='Dev Raw Data Source - Local FS').configure_for_environment(gcp)

    sdc_executor.add_pipeline(pipeline)
    sdc_executor.start_pipeline(pipeline).wait_for_finished()


def read_avro_data(bucket_name, sdc_builder, sdc_executor, gcp):
    # Read Avro data from GCS Origin. This is used to verify that the Avro format has been succesfully stored
    # in test_data_format_avro.
    SCHEMA = {'namespace': 'example.avro',
              'type': 'record',
              'name': 'Employee',
              'fields': [{'name': 'name', 'type': 'string'},
                         {'name': 'age', 'type': 'int'},
                         {'name': 'emails', 'type': {'type': 'array', 'items': 'string'}},
                         {'name': 'boss', 'type': ['Employee', 'null']}]}

    pipeline_builder = sdc_builder.get_pipeline_builder()

    google_cloud_storage = pipeline_builder.add_stage('Google Cloud Storage', type='origin')

    google_cloud_storage.set_attributes(bucket=bucket_name,
                                        common_prefix='gcs-test',
                                        prefix_pattern='**/*',
                                        data_format='AVRO',
                                        avro_schema=json.dumps(SCHEMA),
                                        avro_schema_location='SOURCE')
    wiretap = pipeline_builder.add_wiretap()

    google_cloud_storage >> wiretap.destination

    pipeline = pipeline_builder.build(
        title='Read Avro from GCS Origin').configure_for_environment(gcp)
    sdc_executor.add_pipeline(pipeline)

    logger.info('Starting GCS Origin pipeline and wait until the information is read ...')
    sdc_executor.start_pipeline(pipeline)
    sdc_executor.wait_for_pipeline_metric(pipeline, 'input_record_count', 1)
    sdc_executor.stop_pipeline(pipeline)

    return wiretap.output_records
